---
title: "Cluster: Overview"
sidebar_label: Clustering Overview
sidebar_position: 0
hide_table_of_contents: true
---

import Admonition from '@theme/Admonition';
import Tabs from '@theme/Tabs';
import TabItem from '@theme/TabItem';
import CodeBlock from '@theme/CodeBlock';
import LanguageSwitcher from "@site/src/components/LanguageSwitcher";
import LanguageContent from "@site/src/components/LanguageContent";

# Cluster: Overview
<Admonition type="info" title="Info">

RavenDB's clustering provides redundancy and an increased availability of data that is consistent  
across a fault-tolerant, [High-Availability](https://en.wikipedia.org/wiki/High-availability_cluster) cluster.  
</Admonition>
<Admonition type="note" title="Cluster Topology" id="cluster-topology" href="#cluster-topology">

* A [RavenDB Cluster](../../glossary/ravendb-cluster) consists of one or more RavenDB server instances which are called [Cluster Nodes](../../glossary/cluster-node).  
* Each node has a specific state and type, learn more in [Cluster Topology](../../server/clustering/rachis/cluster-topology).  
</Admonition>

<Admonition type="note" title="Cluster Consensus" id="cluster-consensus" href="#cluster-consensus">

* Some actions, such as creating a new database or creating an index, require a [cluster consensus](../../server/clustering/rachis/consensus-operations) in order to occur.  
* The cluster nodes are kept in consensus by using [Rachis](../../server/clustering/rachis/what-is-rachis), 
  which is RavenDB's Raft Consensus Algorithm implementation for distributed systems.  
* **Rachis** algorithm ensures the following:  
  * These actions are done only if the majority of the nodes in the cluster agreed to it !  
  * Any such series of events (each called a [Raft Command](../../glossary/raft-command)) will be executed in the _same_ order on each node.  
</Admonition>

<Admonition type="note" title="Data Consistency" id="data-consistency" href="#data-consistency">

* In RavenDB, the database is replicated to multiple nodes - see [Database Distribution](../../server/clustering/distribution/distributed-database).  
* A group of nodes in the cluster that contains the same database is called a [Database Group](../../studio/database/settings/manage-database-group).  
  (The number of nodes in the database group is set by the replication factor supplied when creating the database).  
* Documents are kept in sync across the _Database Group_ nodes with a [master to master replication](../../server/clustering/replication/replication).  
* Any document related change such as a CRUD operation doesn't go through Raft, 
  instead, it is automatically **replicated** to the other database instances to in order to keep the data up-to-date.  
</Admonition>

<Admonition type="note" title="Data Availability" id="data-availability" href="#data-availability">

* Due to the consistency of the data, even if the majority of the cluster is down, as long as a single node is available, we can still process Reads and Writes.
* Read requests can be spread among the cluster's nodes for better performance.  
</Admonition>

<Admonition type="note" title="Distributed Work" id="distributed-work" href="#distributed-work">

* Whenever there's a [Work Task](../../server/clustering/distribution/highly-available-tasks) for a _Database Group_ to do (e.g. a Backup task), 
  the cluster will decide which node will actually be responsible for it.  
* These tasks are operational even if the node to which the client is connected to is down, as this nodes' tasks are **re-assigned** to other available nodes in the _Database Group_.  
</Admonition>

<Admonition type="note" title="Cluster's Health" id="cluster-s-health" href="#cluster-s-health">

* The cluster's health is monitored by the [Cluster Observer](../../server/clustering/distribution/cluster-observer) which checks upon each node in the cluster.  
* The node state is recorded in the relevant database groups so that the cluster can maintain the database replication factor and re-distribute its work tasks if needed.  
</Admonition>

<Admonition type="note" title="CPU Core Distribution" id="cpu-core-distribution" href="#cpu-core-distribution">

* A cluster is limited in the maximum number of CPU cores that can be used by all of its nodes 
at a given time.  
* This limit is determined by the cluster's RavenDB license. For example, a Community licensed 
cluster can have up to 3 cores. This means that when there is just one node in the cluster, it 
can use 1-3 cores. If there are 3 nodes, each will use 1 core (since a server is allowed to be 
assigned with at least 1 core).  
* The number of assigned cores is divided evenly between all the nodes of a cluster.  
* The maximum number of cores a node can use can be limited 
  [using Studio](../../studio/cluster/cluster-view#reassign-cores).  
</Admonition>

